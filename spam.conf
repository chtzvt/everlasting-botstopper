##################### Spam Bot Blocking ##########################

# Spam blocker v2.9 by Charlton Trezevant

# Disable Direct IP access to the server
# Enable this if you have a FQDN. This will stop most scanners.
# You may also want to consider redirecting to your FQDN rather than serving a 403.
# This should be enabled but is set to ctis.me's IP by default so things aren't automagically broken if you forget ;)
$HTTP["host"] == "104.131.40.203" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Disable requests with bad hosts.
# This should be a regex containing your FQDN(s).
# Like above, you may want to consider redirecting to your FQDN rather than serving a 403.
# This should be enabled! To do so, fill it in with your domains and uncomment it.
#$HTTP["host"] !~ "ctis\.me|example\.com" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

### Block spam crawlers!
# Empty UA, or -.
#This blocks a lot of spam crawlers and bad robots. 
$HTTP["useragent"] == "" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
$HTTP["useragent"] == "-" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Block bad requsts with empty "Host" headers.
$HTTP["host"] == "" {url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
$HTTP["host"] == "-" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# cURL and Wget/HTTP Libraries commonly used by skiddies (disabled by default).
#$HTTP["useragent"] =~ "curl|Wget" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
#$HTTP["useragent"] =~ "libwww-perl|Apache-HttpClient|Nmap\ Scripting\ Engine|Mozilla\ FireFox\ Test|python-requests" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Deny all URLs to likely malicious UAs (minimizing access to potentially vulnerable apps).
$HTTP["useragent"] =~ "perl|\{|\}|\/var\/|\/tmp\/|\/etc\/|China.Z|ZmEu|Zollard|gawa\.sa\.pilipinas|Jorgee" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Spammy SEO companies
# Some UAs sourced from http://www.iplists.com/non_engines.txt
$HTTP["useragent"] =~ "thunderstone|PageAnalyzer|Netcraft|5118\.com|MJ12bot|majestic12\.co\.uk|dotbot|LSSRocketCrawler|LightspeedSystems|StatsInfo|ips-agent|TurnitinBot|SlySearch|ScoutAbout|RPT-HTTPClient|WebZIP|Webclipping\.com|webrank|websquash\.com|lwp-trivial|AESOP_com_SpiderMan|Zeus|Cyveillance|Lite\ Bot|flunky|Microsoft\ URL\ Control|NAMEPROTECT|NPBot|aipbot|WebFilter|Indy\ Library|SurveyBot|Morfeus\ Fucking\ Scanner|probethenet\.com|digitalmole|AhrefsBot|linkfluence\.net|Kraken|datasift\.com|DomainSigmaCrawler|AdnormCrawler|adnorm\.com|OptimizationCrawler|domainoptima\.com|woriobot|worio\.com|zitebot|coccoc|coccoc.com|Mnogosearch|Synapse" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }
$HTTP["referer"] =~ "semalt|-seo|darodar\.com|makemoneyonline\.com|for-website\.com|for-your-website\.com|social-buttons\.com|free-share-buttons\.com|buy-cheap-online\.info|24x7-allrequestsallowed\.com|domainsigma\.com|for-your|video--production\.com|justprofit\.xyz|hundejo\.com|pizza-tycoon\.com|pizza-imperia\.com|buypillsorderonline\.com" { url.access-deny =  ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Net Scanners, including https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "netscan.lekus.su|pdrlabs.net|massscan|ip138.com|nerd.host" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Some of these IPs are also Baidu, but none of them respect robots.txt and 
# simply change the user-agents of so as to circumvent blocking. Of course, we can fix that :)
# Blocked IPs:
# 123.125.71.x  220.181.108.x 185.10.107.x - Baidu Spiders
# 202.46.*.* - Blocks bots @ ptr.cnsat.com.cn 
# 180.76.15.*, 185.10.107.* - More Baidu
$HTTP["remoteip"] =~ "123\.125\.71\.*|220\.181\.108\.*|202\.46\.*.*|180.76.15.*|185.10.107.*" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Google Crawler Validation (disabled by default, though it does work quite well. Uncomment to enable.)
# Unfortuntely, many spambots impersonate Google's web crawler (Googlebot).
#
# This config attempts to validate requests from Google user agents by comparing
# the ip address of the client to a list of known good IP blocks.
# 
# These IP blocks are retrieved through Google's publicly available lists 
# (https://support.google.com/a/answer/60764?hl=en, http://snurps.blogspot.com/2013/10/how-many-ip-addresses-does-google-have.html)
# with the command dig TXT +short _netblocks{,2,3}.google.com | tr ' ' '\n' | grep '^ip4:'
# Note that the whitelist is purposely as coarse as possible (using /8s) to prevent false negatives, as I do not update this section often. 
# In practice, this leniency has not caused issues, nearly all spammers impersonating google use AWS or are based in small countries not located in these blocks.
#$HTTP["useragent"] =~ "Google|Googlebot" {
#  $HTTP["remoteip"] !~ "64\.*\.*\.*|66\.*\.*\.*|72\.*\.*\.*|74\.*\.*\.*|173\.*\.*\.*|207\.*\.*\.*|209\.*\.*\.*|216\.*\.*\.*|108.*.*.*" {
#    url.access-deny = ( "" )
#    accesslog.filename = "/var/log/lighttpd/spam.log"
#  }
#}

# Baidu, Sogou, YoDao, Haosou, and Yisou, all of whom disregard robots.txt
$HTTP["useragent"] =~ "Baidu|Baiduspider|Sogou|YoudaoBot|YodaoBot|HaoSouSpider|360Spider|YisouSpider" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Always allow access to robots.txt, to everyone.
$HTTP["url"] =~ "^/robots.txt"{ url.access-deny = ("disable") }

##################### END Spam Bot Blocking ##########################
